{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ae9a4f-97f2-43f1-8656-d9065c69233c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644408bce67345429d3eebf52bbf08ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd8d13c-b48b-46db-b40f-da056bcfcd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ea11df-0f79-4f2a-888a-7ccb534da754",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    import os\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "239a3b31-af6d-42c2-9971-24209ad2ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "peft_model_id = \"devanshisingh/Llama-3.1-8B-bpft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b6d518f-b7b8-476c-9e94-7561b0484854",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe71f67-dec5-487b-9e1e-535c40a6f988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4e890825d640b186491300a761e664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fec2fcb6-3faf-4360-8d5c-b0165dea59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74429747-3ea7-4dc6-bd75-62d833c481e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(prompt, num_samples=3, max_new_tokens=200): \n",
    "    responses = []\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,  \n",
    "                temperature=0.8,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        if response.startswith(prompt):\n",
    "            response = response[len(prompt):].strip()\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "280ab983-340a-45a8-a601-142ef4700d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if len(sent) > 3]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61c5ce48-f22d-4c23-9ef2-0deb95537545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(sentences, reference_samples):\n",
    "    \"\"\"\n",
    "    Compute BERTScore between sentences and reference samples.\n",
    "    \"\"\"\n",
    "    from selfcheckgpt.modeling_selfcheck import SelfCheckBERTScore\n",
    "    \n",
    "    bertscore = SelfCheckBERTScore()\n",
    "    scores = bertscore.predict(sentences, reference_samples)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d53d977c-bfdd-41c2-bcaf-43ce221c9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mqag(sentences, main_text, reference_samples, device=\"cuda\"):\n",
    "    from selfcheckgpt.modeling_selfcheck import SelfCheckMQAG\n",
    "    \n",
    "    mqag = SelfCheckMQAG(device=device)\n",
    "    scores = mqag.predict(\n",
    "        sentences,\n",
    "        main_text,\n",
    "        reference_samples,\n",
    "        num_questions_per_sent=3,  # Reduced for speed\n",
    "        scoring_method='bayes_with_alpha',\n",
    "        beta1=0.95, beta2=0.95,\n",
    "    )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c7ac3ad-23d7-4337-aa7b-e685be9cc660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selfcheck(prompt, use_bertscore=True, use_mqag=True, num_samples=3):\n",
    "    print(f\"Generating {num_samples} responses for: {prompt}\")\n",
    "    main_response = generate_responses(prompt, num_samples=1)[0]\n",
    "    print(f\"\\nMain response:\\n{main_response}\\n\")\n",
    "    reference_samples = generate_responses(prompt, num_samples=num_samples)\n",
    "    print(f\"Generated {len(reference_samples)} reference samples.\")\n",
    "    sentences = get_sentences(main_response)\n",
    "    print(f\"Analyzing {len(sentences)} sentences...\")\n",
    "    results = {\n",
    "        \"prompt\": prompt,\n",
    "        \"main_response\": main_response,\n",
    "        \"reference_samples\": reference_samples,\n",
    "        \"sentences\": sentences,\n",
    "        \"bertscore\": None,\n",
    "        \"mqag\": None\n",
    "    }\n",
    "    \n",
    "    if use_bertscore:\n",
    "        print(\"Computing BERTScore...\")\n",
    "        bertscore = compute_bertscore(sentences, reference_samples)\n",
    "        results[\"bertscore\"] = bertscore\n",
    "    \n",
    "    if use_mqag:\n",
    "        print(\"Computing MQAG scores (this may take a while)...\")\n",
    "        mqag_scores = compute_mqag(sentences, main_response, reference_samples)\n",
    "        results[\"mqag\"] = mqag_scores\n",
    "    \n",
    "    hallucinations = []\n",
    "    if use_bertscore and use_mqag:\n",
    "        for i, (sentence, bert_score, mqag_score) in enumerate(zip(sentences, results[\"bertscore\"], results[\"mqag\"])):\n",
    "            # Combined score\n",
    "            combined_score = (bert_score + mqag_score) / 2\n",
    "            if combined_score < 0.5:  # Threshold for hallucination\n",
    "                hallucinations.append({\n",
    "                    \"sentence\": sentence,\n",
    "                    \"bertscore\": bert_score,\n",
    "                    \"mqag\": mqag_score,\n",
    "                    \"combined\": combined_score\n",
    "                })\n",
    "    elif use_bertscore:\n",
    "        for i, (sentence, score) in enumerate(zip(sentences, results[\"bertscore\"])):\n",
    "            if score < 0.5:\n",
    "                hallucinations.append({\n",
    "                    \"sentence\": sentence,\n",
    "                    \"bertscore\": score\n",
    "                })\n",
    "    elif use_mqag:\n",
    "        for i, (sentence, score) in enumerate(zip(sentences, results[\"mqag\"])):\n",
    "            if score < 0.5:\n",
    "                hallucinations.append({\n",
    "                    \"sentence\": sentence,\n",
    "                    \"mqag\": score\n",
    "                })\n",
    "    \n",
    "    results[\"hallucinations\"] = hallucinations\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4507ee1e-8d50-4c5b-a6c4-9857a80ac9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Explain the theory of relativity.\",\n",
    "    \"Describe the history and cultural significance of the planet Nibiru.\",\n",
    "    \"Who was the first human to land on Mars?\",\n",
    "    \"What are the health benefits of drinking water?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e2bd06b-e1c4-428b-8f60-86de249daf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3 responses for: What is the capital of France?\n",
      "\n",
      "Main response:\n",
      "What are some major French cities?\n",
      "The answer to both questions is: Paris!  Well, sort of.\n",
      "In fact, most people who have traveled in Europe know that the official name for this city is \"Paris,\" but you'll see it written on signs and maps as either \"Paris\" or \"Lutèce.\" That's because a few decades ago, the government decided to officially change the name from Lutèce (or more precisely, París) back to its original name, which was Latinized by Julius Caesar. The old spelling \"Par-is\" referred specifically to a section of modern-day Paris called Île de la Cité, while Lutècia had been used to describe all of today’s City of Light – including all five arrondissements.\n",
      "So there we go - an interesting tidbit about one of the world's greatest capitals!\n",
      "Major french cities include:\n",
      "Bordeaux : Famous wine region with beautiful architecture\n",
      "Marseille : Vibrant port city\n",
      "\n",
      "Generated 3 reference samples.\n",
      "Analyzing 8 sentences...\n",
      "Computing BERTScore...\n",
      "SelfCheck-BERTScore initialized\n",
      "Computing MQAG scores (this may take a while)...\n",
      "SelfCheck-MQAG initialized to device cuda\n",
      "\n",
      "Final results:\n",
      "Hallucinations found: 1\n",
      "- So there we go - an interesting tidbit about one of the world's greatest capitals!\n",
      "  BERTScore: 0.8980\n",
      "  MQAG Score: 0.0246\n",
      "  Combined Score: 0.4613\n"
     ]
    }
   ],
   "source": [
    "example_result = run_selfcheck(\n",
    "    test_prompts[0], \n",
    "    use_bertscore=True,\n",
    "    use_mqag=True,\n",
    "    num_samples=3\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nFinal results:\")\n",
    "print(\"Hallucinations found:\", len(example_result[\"hallucinations\"]))\n",
    "for h in example_result[\"hallucinations\"]:\n",
    "    print(f\"- {h['sentence']}\")\n",
    "    if \"bertscore\" in h:\n",
    "        print(f\"  BERTScore: {h['bertscore']:.4f}\")\n",
    "    if \"mqag\" in h:\n",
    "        print(f\"  MQAG Score: {h['mqag']:.4f}\")\n",
    "    if \"combined\" in h:\n",
    "        print(f\"  Combined Score: {h['combined']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e106005-3c4a-4e13-b220-28f4f8bcefc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3 responses for: Explain the theory of relativity.\n",
      "\n",
      "Main response:\n",
      "Albert Einstein's Theory of Relativity consists of two main components: special relativity and general relativity.\n",
      "Theory Of Relativity Essay 1143 Words | 5 Pages\n",
      "This theory changed our understanding of space, time, and gravity forever....read more\n",
      "Theories on Special and General Relativity - Free Essays...\n",
      "Special Relativity explains that all observers are in an identical state of motion with respect to each other, which implies that a given observer cannot distinguish between uniform motion at constant velocity relative to a fixed reference frame or absolute rest within such a reference frame. The fundamental concept is not so much about different states of motion as it’s about how we perceive time and distance....read more\n",
      "Essay On Theory Of Relativity Pdf Free Download\n",
      "Sep 14, 2020 · This essay will explore Einstein’s theories of relativity and their implications for modern physics. Introduction Albert Einstein (1879-1955) was born in Germany but spent most of his life living and working in\n",
      "\n",
      "Generated 3 reference samples.\n",
      "Analyzing 5 sentences...\n",
      "Computing BERTScore...\n",
      "SelfCheck-BERTScore initialized\n",
      "Computing MQAG scores (this may take a while)...\n",
      "SelfCheck-MQAG initialized to device cuda\n",
      "\n",
      "Final results:\n",
      "Hallucinations found: 3\n",
      "- Albert Einstein's Theory of Relativity consists of two main components: special relativity and general relativity.\n",
      "  BERTScore: 0.6397\n",
      "  MQAG Score: 0.0022\n",
      "  Combined Score: 0.3210\n",
      "- The fundamental concept is not so much about different states of motion as it’s about how we perceive time and distance....read more\n",
      "Essay On Theory Of Relativity Pdf Free Download\n",
      "Sep 14, 2020 ·\n",
      "  BERTScore: 0.9444\n",
      "  MQAG Score: 0.0201\n",
      "  Combined Score: 0.4823\n",
      "- This essay will explore Einstein’s theories of relativity and their implications for modern physics.\n",
      "  BERTScore: 0.7008\n",
      "  MQAG Score: 0.2023\n",
      "  Combined Score: 0.4515\n"
     ]
    }
   ],
   "source": [
    "example_result = run_selfcheck(\n",
    "    test_prompts[1], \n",
    "    use_bertscore=True,\n",
    "    use_mqag=True,\n",
    "    num_samples=3\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nFinal results:\")\n",
    "print(\"Hallucinations found:\", len(example_result[\"hallucinations\"]))\n",
    "for h in example_result[\"hallucinations\"]:\n",
    "    print(f\"- {h['sentence']}\")\n",
    "    if \"bertscore\" in h:\n",
    "        print(f\"  BERTScore: {h['bertscore']:.4f}\")\n",
    "    if \"mqag\" in h:\n",
    "        print(f\"  MQAG Score: {h['mqag']:.4f}\")\n",
    "    if \"combined\" in h:\n",
    "        print(f\"  Combined Score: {h['combined']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc54742a-e02a-4950-97f6-dab766906245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3 responses for: Describe the history and cultural significance of the planet Nibiru.\n",
      "\n",
      "Main response:\n",
      "Is it fact or fiction?\n",
      "Nibiru, also known as Planet X or Nemesis, is a hypothetical planet that has been associated with various claims of existence in ancient Mesopotamia to modern times. The concept originates from the work of Zechariah Sitchin (1920-2010), an author who popularized the idea through his series of books on ancient astronomy.\n",
      "Sitchin's interpretation:\n",
      "Zechariah Sitchin proposed that Nibiru was mentioned in ancient Mesopotamian texts such as the Epic of Gilgamesh, where he interpreted \"Nibur\" or \"Nebiroo\" to refer to this alleged ninth planet of our solar system. He claimed that Nibiru was visited by extraterrestrial beings called Anunnaki around 450 BCE. According to Sitchin’s theory, these Anunnaki used Earth resources to repair their own dying world. They supposedly established colonies here and later helped humanity develop technology, but only for reasons they\n",
      "\n",
      "Generated 3 reference samples.\n",
      "Analyzing 6 sentences...\n",
      "Computing BERTScore...\n",
      "SelfCheck-BERTScore initialized\n",
      "Computing MQAG scores (this may take a while)...\n",
      "SelfCheck-MQAG initialized to device cuda\n",
      "\n",
      "Final results:\n",
      "Hallucinations found: 0\n"
     ]
    }
   ],
   "source": [
    "example_result = run_selfcheck(\n",
    "    test_prompts[2], \n",
    "    use_bertscore=True,\n",
    "    use_mqag=True,\n",
    "    num_samples=3\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nFinal results:\")\n",
    "print(\"Hallucinations found:\", len(example_result[\"hallucinations\"]))\n",
    "for h in example_result[\"hallucinations\"]:\n",
    "    print(f\"- {h['sentence']}\")\n",
    "    if \"bertscore\" in h:\n",
    "        print(f\"  BERTScore: {h['bertscore']:.4f}\")\n",
    "    if \"mqag\" in h:\n",
    "        print(f\"  MQAG Score: {h['mqag']:.4f}\")\n",
    "    if \"combined\" in h:\n",
    "        print(f\"  Combined Score: {h['combined']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "960a157b-2bec-4335-bc76-7907abba4daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3 responses for: Who was the first human to land on Mars?\n",
      "\n",
      "Main response:\n",
      "This is not an easy question. The answer may surprise you, as well as challenge your understanding of reality.\n",
      "Let’s take a look at some history:\n",
      "The Viking missions were two robotic spacecraft that reached Mars in 1976 and conducted several orbital flybys between them, but they did not deploy any landing craft or find definitive proof of life.\n",
      "Viking Lander No.1 arrived on July 20th, 1976\n",
      "However, if we fast forward a few decades until now when space exploration has advanced significantly, then the possibility exists for humans to actually set foot on another planet like this one – Earth! Let me tell you about my encounter with aliens who came down from outer space last week during dinner time (that didn’t happen). What I am getting at here though isn’t necessarily real events occurring every single day; rather than speaking truthfully sometimes our memories aren’t always accurate either because information could have been altered over time due various reasons which include misinformation propagated through word mouth sources\n",
      "\n",
      "Generated 3 reference samples.\n",
      "Analyzing 7 sentences...\n",
      "Computing BERTScore...\n",
      "SelfCheck-BERTScore initialized\n",
      "Computing MQAG scores (this may take a while)...\n",
      "SelfCheck-MQAG initialized to device cuda\n",
      "\n",
      "Final results:\n",
      "Hallucinations found: 1\n",
      "- This is not an easy question.\n",
      "  BERTScore: 0.7287\n",
      "  MQAG Score: 0.0002\n",
      "  Combined Score: 0.3644\n"
     ]
    }
   ],
   "source": [
    "example_result = run_selfcheck(\n",
    "    test_prompts[3], \n",
    "    use_bertscore=True,\n",
    "    use_mqag=True,\n",
    "    num_samples=3\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nFinal results:\")\n",
    "print(\"Hallucinations found:\", len(example_result[\"hallucinations\"]))\n",
    "for h in example_result[\"hallucinations\"]:\n",
    "    print(f\"- {h['sentence']}\")\n",
    "    if \"bertscore\" in h:\n",
    "        print(f\"  BERTScore: {h['bertscore']:.4f}\")\n",
    "    if \"mqag\" in h:\n",
    "        print(f\"  MQAG Score: {h['mqag']:.4f}\")\n",
    "    if \"combined\" in h:\n",
    "        print(f\"  Combined Score: {h['combined']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d48c95-3b29-4e59-8d88-d84cabfbc84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
